# CodeSculptor

**CodeSculptor** is an experimental AI-powered development agent designed to **analyze, refactor, and extend Python projects automatically**.
It uses an OpenAI-like plannerâ€“executor loop on top of a [vLLM](https://github.com/vllm-project/vllm) backend, combining project context analysis, structured tool calls, and iterative refinement.

---
## Table of Content
- [CodeSculptor](#codesculptor)
  - [Table of Content](#table-of-content)
  - [ğŸš€ Getting Started / Usage](#-getting-started--usage)
    - [1. Clone the repository](#1-clone-the-repository)
    - [2. Install the package](#2-install-the-package)
    - [3. Set environment variables](#3-set-environment-variables)
    - [4. Run CLI commands](#4-run-cli-commands)
    - [5. Other examples](#5-other-examples)
    - [6. Workflow Overview](#6-workflow-overview)
  - [ğŸš€ Features](#-features)
  - [ğŸ“¦ Repository Structure](#-repository-structure)
  - [âš¡ Quick Setup with DevContainer](#-quick-setup-with-devcontainer)
    - [1. DevContainer](#1-devcontainer)
    - [2. Dockerfile](#2-dockerfile)
    - [3. Python Dependencies](#3-python-dependencies)
    - [4. Start the DevContainer](#4-start-the-devcontainer)
    - [5. Using the Agent](#5-using-the-agent)
  - [âš¡ Usage](#-usage)
    - [Workflow](#workflow)
  - [ğŸ› ï¸ Tools Available](#ï¸-tools-available)
  - [ğŸ‘¨â€ğŸ’» Coding](#-coding)
    - [Development Setup](#development-setup)
    - [How It Works](#how-it-works)
  - [ğŸŒ Roadmap](#-roadmap)
  - [ğŸ“„ License](#-license)


---

## ğŸš€ Getting Started / Usage

Follow these steps to install and use **CodeSculptor**:

### 1. Clone the repository

```bash
git clone https://github.com/Perpetue237/codesculptor.git
cd codesculptor
```

### 2. Install the package

Use editable mode to allow local changes:

```bash
pip install -e .
```

### 3. Set environment variables

CodeSculptor requires a running vLLM server. Set:

```bash
export VLLM_URL="http://localhost:8008"
export VLLM_MODEL="openai/gpt-oss-120b"
```

Adjust according to your server setup.

### 4. Run CLI commands

Generate or refactor code with `codesculptor-cli`. For example, to create a basic Dockerized FastAPI app:

```bash
codesculptor-cli ./test_project "create files for a basic dockerized FastAPI application. The initial app should just return a JSON with 'hello to the OpenAI community'"
```

This will:

* Generate a minimal FastAPI app structure.
* Create Dockerfiles for containerization.
* Initialize the project in `./test_project`.

### 5. Other examples

* Merge multiple Python modules into a single file.
* Refactor functions and automatically generate unit tests.
* Update imports across the project after refactoring.

### 6. Workflow Overview

1. `prepare_context` scans your project.
2. `PlannerAgent` outputs a structured JSON plan.
3. `AgentLoop` executes each tool step by step.
4. Tests are run automatically (existing or generated).
5. Changes are applied safely, with backups if needed.


## ğŸš€ Features

* ğŸ“‚ **Project Context Builder** â€“ Parses files, functions, classes, and imports.
* ğŸ§  **Planner Agent** â€“ Generates structured JSON plans of tool calls (no free-form text).
* ğŸ” **Agent Loop** â€“ Executes tool calls, tracks logs, and retries if needed.
* ğŸ› ï¸ **Tool Registry** â€“ Includes file creation, backup, import updates, code refactoring, and test execution.
* ğŸ§ª **Automated Testing** â€“ Runs `pytest` or generated test files before/after modifications.
* ğŸ–‹ï¸ **Code Refactoring** â€“ Uses LLM guidance with file structure hints for safer transformations.

---

## ğŸ“¦ Repository Structure

```
.
â”œâ”€â”€ main.py                # CLI entrypoint
â”œâ”€â”€ agent/                 # Core agent logic
â”‚   â”œâ”€â”€ planner.py         # PlannerAgent â†’ generates tool calls
â”‚   â”œâ”€â”€ loop.py            # AgentLoop â†’ executes plans
â”‚   â””â”€â”€ executor.py        # (reserved for future execution abstraction)
â”œâ”€â”€ llm/                   # vLLM client and prompts
â”‚   â”œâ”€â”€ client.py
â”‚   â””â”€â”€ prompts.py
â”œâ”€â”€ tools/                 # Actionable tools
â”‚   â”œâ”€â”€ prepare_context.py # Builds project context
â”‚   â”œâ”€â”€ refactor_code.py
â”‚   â”œâ”€â”€ run_tests.py
â”‚   â”œâ”€â”€ update_imports.py
â”‚   â””â”€â”€ registry.py
â”œâ”€â”€ utils/                 # Utilities
â”‚   â”œâ”€â”€ file_ops.py
â”‚   â”œâ”€â”€ logging.py
â”‚   â””â”€â”€ search.py
â”œâ”€â”€ test_project/           # Dockerized test project
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/main.py
â””â”€â”€ tests/                  # Reserved for unit/integration tests
```

---

## âš¡ Quick Setup with DevContainer

This repository provides a **VS Code DevContainer** for an optimized development environment with GPU support and vLLM.

### 1. DevContainer

Create `.devcontainer/devcontainer.json`:

```json
{
  "name": "gpt-oss-120b-vllm",
  "build": {
    "dockerfile": "Dockerfile"
  },
  "runArgs": [
    "--gpus", "\"device=5,6\"",
    "--shm-size=64gb",
    "--ulimit", "memlock=-1",
    "--ulimit", "stack=67108864",
    "--ipc=host"
  ],
  "containerEnv": {
    "PYTHONUNBUFFERED": "1"
  },
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-toolsai.jupyter",
        "innerlee.nvidia-smi",
        "Leonardo16.nvidia-gpu",
        "RSIP-Vision.nvidia-smi-plus"
      ]
    }
  },
  "mounts": [
    "source=/raid/vllm,target=/app/vllm,type=bind,consistency=cached"
  ],
  "postStartCommand": "vllm serve openai/gpt-oss-120b --gpu-memory-utilization 0.95 --tensor-parallel-size 2 --download-dir /app/vllm --port 8008"
}
```

---

### 2. Dockerfile

Create `.devcontainer/Dockerfile`:

```dockerfile
FROM vllm/vllm-openai:gptoss

ENV USERNAME="$USERNAME"
ARG DEBIAN_FRONTEND=noninteractive
ENV TZ Europe/Berlin
ENV VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1

COPY . .

RUN pip3 --no-cache-dir install -r requirements.txt
RUN apt-get update && apt-get install -y curl

EXPOSE 8008
```

---

### 3. Python Dependencies

`requirements.txt`:

```
black
pytest
```

Install dependencies inside the container:

```bash
pip install -r requirements.txt
```

---

### 4. Start the DevContainer

1. Open the project in **VS Code**.
2. Click **Reopen in Container** when prompted.
3. The `postStartCommand` will automatically launch **vLLM server** on port `8008`.

---

### 5. Using the Agent

Inside the container:

```bash
 python3 main.py test_project "add comments to all my python files"
```

The agent will use **vLLM** to plan, refactor, and test your project automatically.

---

## âš¡ Usage

Run the CLI with:

```bash
python main.py <project_path> "<user_request>"
```

Example:

```bash
python main.py ./app "Refactor section1.py to extract helper functions"
```

### Workflow

1. `prepare_context` scans your project files.
2. `PlannerAgent` generates a JSON plan of tool calls.
3. `AgentLoop` executes each tool in sequence.
4. Tests run (if available or auto-generated).
5. Changes are applied safely (backup first).

---

## ğŸ› ï¸ Tools Available

* **create\_file** â†’ Create new source files.
* **backup\_file** â†’ Backup existing files before modifications.
* **update\_imports** â†’ Fix imports across files.
* **refactor\_code** â†’ Apply structured code transformations.
* **run\_tests** â†’ Execute test suite or generated tests.
* **format\_code** â†’ Run `black` to auto-format.

---

## ğŸ‘¨â€ğŸ’» Coding

### Development Setup

Install dependencies:

```bash
pip install -r test_project/requirements.txt
```

Run tests:

```bash
pytest
```

Format code:

```bash
black .
```

### How It Works

The agent follows an **OpenAI-like loop**:

1. **Planner Agent** â€“ Receives user request + context â†’ outputs JSON plan.
2. **Agent Loop** â€“ Executes steps sequentially, ensuring dependencies are respected.
3. **Tools** â€“ Low-level operations (create, backup, refactor, imports, tests).
4. **Re-Planning** (future) â€“ If tests fail, the agent can retry with initial context.

---

## ğŸŒ Roadmap

*

---

## ğŸ“„ License

MIT License. See [LICENSE](LICENSE) for details.
