# CodeSculptor

**CodeSculptor** is an experimental AI-powered development agent designed to **analyze, refactor, and extend Python projects automatically**.
It uses an OpenAI-like plannerâ€“executor loop on top of a [vLLM](https://github.com/vllm-project/vllm) backend, combining project context analysis, structured tool calls, and iterative refinement.

---

## ğŸš€ Features

* ğŸ“‚ **Project Context Builder** â€“ Parses files, functions, classes, and imports.
* ğŸ§  **Planner Agent** â€“ Generates structured JSON plans of tool calls (no free-form text).
* ğŸ” **Agent Loop** â€“ Executes tool calls, tracks logs, and retries if needed.
* ğŸ› ï¸ **Tool Registry** â€“ Includes file creation, backup, import updates, code refactoring, and test execution.
* ğŸ§ª **Automated Testing** â€“ Runs `pytest` or generated test files before/after modifications.
* ğŸ–‹ï¸ **Code Refactoring** â€“ Uses LLM guidance with file structure hints for safer transformations.

---

## ğŸ“¦ Repository Structure

```
.
â”œâ”€â”€ main.py                # CLI entrypoint
â”œâ”€â”€ agent/                 # Core agent logic
â”‚   â”œâ”€â”€ planner.py         # PlannerAgent â†’ generates tool calls
â”‚   â”œâ”€â”€ loop.py            # AgentLoop â†’ executes plans
â”‚   â””â”€â”€ executor.py        # (reserved for future execution abstraction)
â”œâ”€â”€ llm/                   # vLLM client and prompts
â”‚   â”œâ”€â”€ client.py
â”‚   â””â”€â”€ prompts.py
â”œâ”€â”€ tools/                 # Actionable tools
â”‚   â”œâ”€â”€ prepare_context.py # Builds project context
â”‚   â”œâ”€â”€ refactor_code.py
â”‚   â”œâ”€â”€ run_tests.py
â”‚   â”œâ”€â”€ update_imports.py
â”‚   â””â”€â”€ registry.py
â”œâ”€â”€ utils/                 # Utilities
â”‚   â”œâ”€â”€ file_ops.py
â”‚   â”œâ”€â”€ logging.py
â”‚   â””â”€â”€ search.py
â”œâ”€â”€ app/                   # Example application (modifiable by agent)
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ section1.py
â”‚   â”œâ”€â”€ section2.py
â”‚   â””â”€â”€ section3.py
â”œâ”€â”€ test_project/           # Dockerized test project
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/main.py
â””â”€â”€ tests/                  # Reserved for unit/integration tests
```

---

## âš¡ Quick Setup with DevContainer

This repository provides a **VS Code DevContainer** for an optimized development environment with GPU support and vLLM.

### 1. DevContainer

Create `.devcontainer/devcontainer.json`:

```json
{
  "name": "gpt-oss-120b-vllm",
  "build": {
    "dockerfile": "Dockerfile"
  },
  "runArgs": [
    "--gpus", "\"device=5,6\"",
    "--shm-size=64gb",
    "--ulimit", "memlock=-1",
    "--ulimit", "stack=67108864",
    "--ipc=host"
  ],
  "containerEnv": {
    "PYTHONUNBUFFERED": "1"
  },
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-toolsai.jupyter",
        "innerlee.nvidia-smi",
        "Leonardo16.nvidia-gpu",
        "RSIP-Vision.nvidia-smi-plus"
      ]
    }
  },
  "mounts": [
    "source=/raid/vllm,target=/app/vllm,type=bind,consistency=cached"
  ],
  "postStartCommand": "vllm serve openai/gpt-oss-120b --gpu-memory-utilization 0.95 --tensor-parallel-size 2 --download-dir /app/vllm --port 8008"
}
```

---

### 2. Dockerfile

Create `.devcontainer/Dockerfile`:

```dockerfile
FROM vllm/vllm-openai:gptoss

ENV USERNAME="$USERNAME"
ARG DEBIAN_FRONTEND=noninteractive
ENV TZ Europe/Berlin
ENV VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1

COPY . .

RUN pip3 --no-cache-dir install -r requirements.txt
RUN apt-get update && apt-get install -y curl

EXPOSE 8008
```

---

### 3. Python Dependencies

`requirements.txt`:

```
black
pytest
```

Install dependencies inside the container:

```bash
pip install -r requirements.txt
```

---

### 4. Start the DevContainer

1. Open the project in **VS Code**.
2. Click **Reopen in Container** when prompted.
3. The `postStartCommand` will automatically launch **vLLM server** on port `8008`.

---

### 5. Using the Agent

Inside the container:

```bash
python main.py ./app "Refactor section1.py to extract helper functions"
```

The agent will use **vLLM** to plan, refactor, and test your project automatically.

---

## âš¡ Usage

Run the CLI with:

```bash
python main.py <project_path> "<user_request>"
```

Example:

```bash
python main.py ./app "Refactor section1.py to extract helper functions"
```

### Workflow

1. `prepare_context` scans your project files.
2. `PlannerAgent` generates a JSON plan of tool calls.
3. `AgentLoop` executes each tool in sequence.
4. Tests run (if available or auto-generated).
5. Changes are applied safely (backup first).

---

## ğŸ› ï¸ Tools Available

* **create\_file** â†’ Create new source files.
* **backup\_file** â†’ Backup existing files before modifications.
* **update\_imports** â†’ Fix imports across files.
* **refactor\_code** â†’ Apply structured code transformations.
* **run\_tests** â†’ Execute test suite or generated tests.
* **format\_code** â†’ Run `black` to auto-format.

---

## ğŸ‘¨â€ğŸ’» Coding

### Development Setup

Install dependencies:

```bash
pip install -r test_project/requirements.txt
```

Run tests:

```bash
pytest
```

Format code:

```bash
black .
```

### How It Works

The agent follows an **OpenAI-like loop**:

1. **Planner Agent** â€“ Receives user request + context â†’ outputs JSON plan.
2. **Agent Loop** â€“ Executes steps sequentially, ensuring dependencies are respected.
3. **Tools** â€“ Low-level operations (create, backup, refactor, imports, tests).
4. **Re-Planning** (future) â€“ If tests fail, the agent can retry with initial context.

---

## ğŸŒ Roadmap

*

---

## ğŸ“„ License

MIT License. See [LICENSE](LICENSE) for details.
